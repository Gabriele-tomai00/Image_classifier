{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#librerie\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "zQrdcdKdwhs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione per leggere le immagini e creare una lista di etichette\n",
        "def carica_immagini_e_etichette(cartella_principale):\n",
        "    etichette = []\n",
        "    immagini = []\n",
        "\n",
        "    # Scorri le sottocartelle nella cartella principale\n",
        "    for cartella in os.listdir(cartella_principale):\n",
        "        cartella_path = os.path.join(cartella_principale, cartella)\n",
        "\n",
        "        # Verifica che sia una cartella\n",
        "        if os.path.isdir(cartella_path):\n",
        "            # Scorri le immagini all'interno della cartella\n",
        "            for file in os.listdir(cartella_path):\n",
        "                file_path = os.path.join(cartella_path, file)\n",
        "\n",
        "                # Verifica che il file sia un'immagine (controlla l'estensione)\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "                    try:\n",
        "                        # Apre l'immagine per verificarne la validità\n",
        "                        img = Image.open(file_path)\n",
        "                        img.verify()  # Verifica se l'immagine è valida\n",
        "\n",
        "                        # Aggiungi l'immagine e la sua etichetta (nome della cartella)\n",
        "                        immagini.append(file_path)\n",
        "                        etichette.append(cartella)\n",
        "                    except (IOError, SyntaxError):\n",
        "                        # Se l'immagine non è valida, la ignora\n",
        "                        continue\n",
        "\n",
        "    return immagini, etichette\n"
      ],
      "metadata": {
        "id": "E6bCrIU3pxTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build a visual vocabulary:\n",
        "\n",
        "\n",
        "*   sample many (10K to 100K) SIFT descriptors from the images of\n",
        "the training set (you either use a detector or sample on a grid in the\n",
        "scale-space);\n",
        "*  cluster them using k-means (the choice of the number of clusters is\n",
        "up to you, and you should experiment with different values, but you\n",
        "could start with a few dozens);\n",
        "*  collect (and save for future use) the clusters’ centroids which repre-\n",
        "sent the k 128-dimensional visual words."
      ],
      "metadata": {
        "id": "56smwoR6vitn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#carico immagini training\n",
        "cartella_training = '/home/eva/Scrivania/esame_cv/train'\n",
        "images_training, train_labels  = carica_immagini_e_etichette(cartella_training) #metti la path cartella"
      ],
      "metadata": {
        "id": "60la1dMvfPdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZVgKqVTvhh1"
      },
      "outputs": [],
      "source": [
        "#SIFT descriptor\n",
        "all_descriptors = []\n",
        "k_p = 10000 #prova\n",
        "for image_path in images_training:\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    sift = cv2.SIFT_create(k_p)\n",
        "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "    if descriptors is not None:\n",
        "        all_descriptors.append(descriptors)\n",
        "all_descriptors = np.vstack(all_descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cluster descriptors per visual words (vocabulary)\n",
        "k = 60 #tuning necessario\n",
        "kmeans = KMeans(n_clusters=k)\n",
        "kmeans.fit(all_descriptors)\n",
        "visual_words = kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "WSkMKA8Rvvy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Represent each image of the training set as a normalized histogram having\n",
        "k bins, each corresponding to a visual word; a possibility is to perform a\n",
        "rather dense sampling in space and scale; another possibility is to use the\n",
        "3SIFT detector to find the points in scale-space where the descriptor is\n",
        "computed. In any case, each computed descriptor will increase the value\n",
        "of the bin corresponding to the closest visual word."
      ],
      "metadata": {
        "id": "MROkd11Fv89D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#da ottimizzare il codice\n",
        "train_histograms = []\n",
        "for image_path in images_training:\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    sift = cv2.SIFT_create(k_p) # non so se sia necessario ricalcolare il sift e se serva lo stesso numero di punti\n",
        "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "\n",
        "    hist = np.zeros(k)\n",
        "    if descriptors is not None:\n",
        "        for descriptor in descriptors:\n",
        "            distances = np.linalg.norm(visual_words - descriptor, axis=1)\n",
        "            visual_word_index = np.argmin(distances)\n",
        "            hist[visual_word_index] += 1\n",
        "\n",
        "    hist = hist / np.sum(hist)  #normalizzo l'istogramma\n",
        "    train_histograms.append(hist)"
      ],
      "metadata": {
        "id": "mq29wJI-wFiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Employ a nearest neighbor classifier and evaluate its performance:\n",
        "• compute the normalized histogram for the test image to be classified;\n",
        "• assign to the image the class corresponding to the training image\n",
        "having the closest histogram.\n",
        "• repeat for all the test images and build a confusion matrix."
      ],
      "metadata": {
        "id": "3WP8k7T2wF-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Carico le immagini di test\n",
        "cartella_test = '/home/eva/Scrivania/esame_cv/test'\n",
        "images_test, test_labels  = carica_immagini_e_etichette(cartella_test)"
      ],
      "metadata": {
        "id": "TdkSqnWgwNcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#istogrammi test ---- nota: ho scritto la versione lunga del codice, meglio definire funzioni apposite, forse ce n'è una in opncv\n",
        "test_histograms = []\n",
        "for image_path in images_test:\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    sift = cv2.SIFT_create(k_p) # al solito uso sempre lo stesso numero di punti per adesso\n",
        "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "\n",
        "    hist = np.zeros(k)\n",
        "    if descriptors is not None:\n",
        "        for descriptor in descriptors:\n",
        "            distances = np.linalg.norm(visual_words - descriptor, axis=1)\n",
        "            visual_word_index = np.argmin(distances)\n",
        "            hist[visual_word_index] += 1\n",
        "\n",
        "    hist = hist / np.sum(hist)  #normalizzo l'istogramma\n",
        "    test_histograms.append(hist)"
      ],
      "metadata": {
        "id": "06CInYJIi-C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#knn non è efficiente si deve cambiare algoritmo poi vediamo cosa usare\n",
        "knn = NearestNeighbors(n_neighbors=24) #ho usato un numero a caso\n",
        "knn.fit(train_histograms)\n",
        "\n",
        "# Find the nearest neighbors and classify based on the closest training image\n",
        "predictions = knn.kneighbors(test_histograms, return_distance=False)\n",
        "predicted_labels = [train_labels[i[0]] for i in predictions]"
      ],
      "metadata": {
        "id": "p1qxF12gmW4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm) #dopo ci metto il grafico carino"
      ],
      "metadata": {
        "id": "zE5kbGqsnVuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a multiclass linear Support Vector Machine, using the one-vs-rest\n",
        "approach (you will need to train 15 binary classifiers having the normalized\n",
        "histograms as the input vectors and positive labels for the “one” class and\n",
        "negative for the “rest.”)"
      ],
      "metadata": {
        "id": "V5mhHqCewNy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(kernel='linear', random_state=42)\n",
        "ovr_classifier = OneVsRestClassifier(svm)\n",
        "x = np.vstack(train_histograms)\n",
        "y = np.array(train_labels)\n",
        "ovr_classifier.fit(x, train_labels)"
      ],
      "metadata": {
        "id": "Of2lH_ZHwT03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluate the multiclass SVM:\n",
        "• compute the normalized histogram for the test image to be classified;\n",
        "• compute the real-valued output of each of the SVMs, using that his-\n",
        "togram as input;\n",
        "• assign to the image the class corresponding to the SVM having the\n",
        "greatest real-valued output.\n",
        "• repeat for all the test images and build a confusion matrix."
      ],
      "metadata": {
        "id": "eq6s_hxLwUJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#devo riguardare meglio questo punto\n",
        "test = np.vstack(test_histograms)\n",
        "y_pred = ovr_classifier.predict(test)\n",
        "y_test = np.array(test_labels)\n",
        "accuracy = accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Vcr66DWWwZ4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix ---- finisco dopo"
      ],
      "metadata": {
        "id": "KMvsmBwYyQx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXRA\n",
        "6. svm cambio kernel"
      ],
      "metadata": {
        "id": "g-lUAdla1AOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dovrebbe essere una cosa simile ma devo provarla meglio\n",
        "from sklearn.metrics.pairwise import pairwise_kernels\n",
        "\n",
        "#gaussiana generalizzata\n",
        "def chi_squared_kernel(X, Y=None, gamma=1.0):\n",
        "    if Y is None:\n",
        "        Y = X\n",
        "\n",
        "    #chi^2\n",
        "    diff = X[:, np.newaxis, :] - Y[np.newaxis, :, :]\n",
        "    sum_X = X[:, np.newaxis, :] + Y[np.newaxis, :, :]\n",
        "    chi_squared = np.sum((diff**2) / (sum_X + 1e-10), axis=2)\n",
        "\n",
        "    # Generalized Gaussian kernel, using the chi-squared distance\n",
        "    return np.exp(-gamma * chi_squared)\n",
        "\n",
        "svm = SVC(kernel='precomputed')"
      ],
      "metadata": {
        "id": "IpXV5ykW1F-7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}